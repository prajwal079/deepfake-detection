{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from pylab import *\n",
    "import re\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Error Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ela_image(path, quality):\n",
    "    temp_filename = 'temp_file_name.jpg'\n",
    "    ela_filename = 'temp_ela.png'\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    image.save(temp_filename, 'JPEG', quality = quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    \n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    if max_diff == 0:\n",
    "        max_diff = 1\n",
    "    scale = 255.0 / max_diff\n",
    "    \n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    return ela_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path):\n",
    "    return np.array(convert_to_ela_image(image_path, 90).resize(image_size)).flatten() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # ELA converted images\n",
    "Y = [] # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are doing ELA for Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 images\n",
      "Processing 1000 images\n",
      "Processing 1500 images\n",
      "Processing 2000 images\n",
      "Processing 2500 images\n",
      "Processing 3000 images\n",
      "Processing 3500 images\n",
      "Processing 4000 images\n",
      "Processing 4500 images\n",
      "Processing 5000 images\n",
      "Processing 5500 images\n",
      "Processing 6000 images\n",
      "Processing 6500 images\n",
      "Processing 7000 images\n",
      "Processing 7500 images\n",
      "Processing 8000 images\n",
      "Processing 8500 images\n",
      "Processing 9000 images\n",
      "Processing 9500 images\n",
      "Processing 10000 images\n",
      "Processing 10500 images\n",
      "Processing 11000 images\n",
      "Processing 11500 images\n",
      "Processing 12000 images\n",
      "Processing 12500 images\n",
      "Processing 13000 images\n",
      "Processing 13500 images\n",
      "Processing 14000 images\n",
      "Processing 14500 images\n",
      "Processing 15000 images\n",
      "Processing 15500 images\n",
      "Processing 16000 images\n",
      "Processing 16500 images\n",
      "Processing 17000 images\n",
      "Processing 17500 images\n",
      "Processing 18000 images\n",
      "Processing 18500 images\n",
      "Processing 19000 images\n",
      "Processing 19500 images\n",
      "Processing 20000 images\n",
      "Processing 20500 images\n",
      "Processing 21000 images\n",
      "Processing 21500 images\n",
      "Processing 22000 images\n",
      "Processing 22500 images\n",
      "Processing 23000 images\n",
      "Processing 23500 images\n",
      "Processing 24000 images\n",
      "Processing 24500 images\n",
      "Processing 25000 images\n",
      "Processing 25500 images\n",
      "Processing 26000 images\n",
      "Processing 26500 images\n",
      "Processing 27000 images\n",
      "Processing 27500 images\n",
      "Processing 28000 images\n",
      "Processing 28500 images\n",
      "Processing 29000 images\n",
      "Processing 29500 images\n",
      "Processing 30000 images\n",
      "Processing 30500 images\n",
      "Processing 31000 images\n",
      "Processing 31500 images\n",
      "Processing 32000 images\n",
      "Processing 32500 images\n",
      "Processing 33000 images\n",
      "Processing 33500 images\n",
      "Processing 34000 images\n",
      "Processing 34500 images\n",
      "Processing 35000 images\n",
      "Processing 35500 images\n",
      "Processing 36000 images\n",
      "Processing 36500 images\n",
      "Processing 37000 images\n",
      "Processing 37500 images\n",
      "Processing 38000 images\n",
      "Processing 38500 images\n",
      "Processing 39000 images\n",
      "Processing 39500 images\n",
      "Processing 40000 images\n",
      "Processing 40500 images\n",
      "Processing 41000 images\n",
      "Processing 41500 images\n",
      "Processing 42000 images\n",
      "Processing 42500 images\n",
      "Processing 43000 images\n",
      "Processing 43500 images\n",
      "Processing 44000 images\n",
      "Processing 44500 images\n",
      "Processing 45000 images\n",
      "Processing 45500 images\n",
      "Processing 46000 images\n",
      "Processing 46500 images\n",
      "Processing 47000 images\n",
      "Processing 47500 images\n",
      "Processing 48000 images\n",
      "Processing 48500 images\n",
      "Processing 49000 images\n",
      "Processing 49500 images\n",
      "Processing 50000 images\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "path = 'Data/real2/'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('jpg') or filename.endswith('png'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(1)\n",
    "            if len(Y) % 10000 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "\n",
    "#random.shuffle(X)\n",
    "#X = X[:2100]\n",
    "#Y = Y[:2100]\n",
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ELA for Fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 50500 images\n",
      "Processing 51000 images\n",
      "Processing 51500 images\n",
      "Processing 52000 images\n",
      "Processing 52500 images\n",
      "Processing 53000 images\n",
      "Processing 53500 images\n",
      "Processing 54000 images\n",
      "Processing 54500 images\n",
      "Processing 55000 images\n",
      "Processing 55500 images\n",
      "Processing 56000 images\n",
      "Processing 56500 images\n",
      "Processing 57000 images\n",
      "Processing 57500 images\n",
      "Processing 58000 images\n",
      "Processing 58500 images\n",
      "Processing 59000 images\n",
      "Processing 59500 images\n",
      "Processing 60000 images\n",
      "Processing 60500 images\n",
      "Processing 61000 images\n",
      "Processing 61500 images\n",
      "Processing 62000 images\n",
      "Processing 62500 images\n",
      "Processing 63000 images\n",
      "Processing 63500 images\n",
      "Processing 64000 images\n",
      "Processing 64500 images\n",
      "Processing 65000 images\n",
      "Processing 65500 images\n",
      "Processing 66000 images\n",
      "Processing 66500 images\n",
      "Processing 67000 images\n",
      "Processing 67500 images\n",
      "Processing 68000 images\n",
      "Processing 68500 images\n",
      "Processing 69000 images\n",
      "Processing 69500 images\n",
      "Processing 70000 images\n",
      "Processing 70500 images\n",
      "Processing 71000 images\n",
      "Processing 71500 images\n",
      "Processing 72000 images\n",
      "Processing 72500 images\n",
      "Processing 73000 images\n",
      "Processing 73500 images\n",
      "Processing 74000 images\n",
      "Processing 74500 images\n",
      "Processing 75000 images\n",
      "Processing 75500 images\n",
      "Processing 76000 images\n",
      "Processing 76500 images\n",
      "Processing 77000 images\n",
      "Processing 77500 images\n",
      "Processing 78000 images\n",
      "Processing 78500 images\n",
      "Processing 79000 images\n",
      "Processing 79500 images\n",
      "Processing 80000 images\n",
      "Processing 80500 images\n",
      "Processing 81000 images\n",
      "Processing 81500 images\n",
      "Processing 82000 images\n",
      "Processing 82500 images\n",
      "Processing 83000 images\n",
      "Processing 83500 images\n",
      "Processing 84000 images\n",
      "Processing 84500 images\n",
      "Processing 85000 images\n",
      "Processing 85500 images\n",
      "Processing 86000 images\n",
      "Processing 86500 images\n",
      "Processing 87000 images\n",
      "Processing 87500 images\n",
      "Processing 88000 images\n",
      "Processing 88500 images\n",
      "Processing 89000 images\n",
      "Processing 89500 images\n",
      "Processing 90000 images\n",
      "Processing 90500 images\n",
      "Processing 91000 images\n",
      "Processing 91500 images\n",
      "Processing 92000 images\n",
      "Processing 92500 images\n",
      "Processing 93000 images\n",
      "Processing 93500 images\n",
      "Processing 94000 images\n",
      "Processing 94500 images\n",
      "Processing 95000 images\n",
      "Processing 95500 images\n",
      "Processing 96000 images\n",
      "Processing 96500 images\n",
      "Processing 97000 images\n",
      "Processing 97500 images\n",
      "Processing 98000 images\n",
      "Processing 98500 images\n",
      "Processing 99000 images\n",
      "Processing 99500 images\n",
      "Processing 100000 images\n",
      "100000 100000\n"
     ]
    }
   ],
   "source": [
    "path = 'data/fake2/'\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('jpg') or filename.endswith('png'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(0)\n",
    "            if len(Y) % 10000 == 0:\n",
    "                print(f'Processing {len(Y)} images')\n",
    "\n",
    "print(len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80\n",
      "20 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "# Create a directory to store the processed batches\n",
    "os.makedirs('processed_batches', exist_ok=True)\n",
    "\n",
    "# Process the data in batches and save each batch separately\n",
    "batch_size = 1000\n",
    "num_batches = (len(X) + batch_size - 1) // batch_size\n",
    "for batch_num in range(num_batches):\n",
    "    batch_start = batch_num * batch_size\n",
    "    batch_end = min(batch_start + batch_size, len(X))\n",
    "    X_batch = np.array(X[batch_start:batch_end])\n",
    "    Y_batch = np.array(Y[batch_start:batch_end])\n",
    "    X_batch = X_batch.reshape(-1, 128, 128, 3)\n",
    "    Y_batch = to_categorical(Y_batch, 2)\n",
    "    np.save(f'processed_batches/X_batch_{batch_num}.npy', X_batch)\n",
    "    np.save(f'processed_batches/Y_batch_{batch_num}.npy', Y_batch)\n",
    "\n",
    "# Get the list of processed batch files\n",
    "X_files = sorted([f for f in os.listdir('processed_batches') if f.startswith('X_batch_')])\n",
    "Y_files = sorted([f for f in os.listdir('processed_batches') if f.startswith('Y_batch_')])\n",
    "\n",
    "# Perform the train-test split on the batch files\n",
    "train_files, val_files = train_test_split(list(zip(X_files, Y_files)), test_size=0.2, random_state=5)\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, batch_files, batch_size):\n",
    "        self.batch_files = batch_files\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_file, Y_file = self.batch_files[index]\n",
    "        X_batch = np.load(f'processed_batches/{X_file}')\n",
    "        Y_batch = np.load(f'processed_batches/{Y_file}')\n",
    "        return X_batch, Y_batch\n",
    "\n",
    "# Create separate generators for training and validation\n",
    "train_generator = DataGenerator(train_files, batch_size)\n",
    "val_generator = DataGenerator(val_files, batch_size)\n",
    "\n",
    "# Print the lengths of the training and validation sets\n",
    "print(len(train_generator), len(train_generator))\n",
    "print(len(val_generator), len(val_generator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_processed, Y_processed, test_size=0.2, random_state=5)\n",
    "# X = X.reshape(-1,1,1,1)\n",
    "# print(len(X_train), len(Y_train))\n",
    "# print(len(X_val), len(Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), padding = 'valid', activation = 'relu', input_shape = (128, 128, 3)))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 124, 124, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 62, 62, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 62, 62, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 123008)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               31490304  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,493,250\n",
      "Trainable params: 31,493,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-3\n",
    "optimizer = Adam(learning_rate = init_lr, decay = init_lr/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 08:42:22.990157: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 329s 4s/step - loss: 0.6944 - accuracy: 0.4719 - val_loss: 0.6986 - val_accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 304s 4s/step - loss: 0.6958 - accuracy: 0.4433 - val_loss: 0.6939 - val_accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 316s 4s/step - loss: 0.6930 - accuracy: 0.5250 - val_loss: 0.6933 - val_accuracy: 0.4000\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 339s 4s/step - loss: 0.6917 - accuracy: 0.5242 - val_loss: 0.6935 - val_accuracy: 0.4000\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 336s 4s/step - loss: 0.6950 - accuracy: 0.5208 - val_loss: 0.6929 - val_accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 331s 4s/step - loss: 0.6897 - accuracy: 0.5250 - val_loss: 0.6930 - val_accuracy: 0.4004\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 336s 4s/step - loss: 0.7438 - accuracy: 0.5307 - val_loss: 0.6902 - val_accuracy: 0.5701\n",
      "Epoch 8/20\n",
      "80/80 [==============================] - 334s 4s/step - loss: 0.6926 - accuracy: 0.5713 - val_loss: 0.6831 - val_accuracy: 0.6483\n",
      "Epoch 9/20\n",
      "80/80 [==============================] - 335s 4s/step - loss: 0.6803 - accuracy: 0.5865 - val_loss: 0.6678 - val_accuracy: 0.6483\n",
      "Epoch 10/20\n",
      "80/80 [==============================] - 335s 4s/step - loss: 0.6829 - accuracy: 0.5852 - val_loss: 0.6579 - val_accuracy: 0.6115\n",
      "Epoch 11/20\n",
      "80/80 [==============================] - 336s 4s/step - loss: 0.6775 - accuracy: 0.5884 - val_loss: 0.6652 - val_accuracy: 0.6407\n",
      "Epoch 12/20\n",
      "80/80 [==============================] - 340s 4s/step - loss: 0.6567 - accuracy: 0.6213 - val_loss: 0.6470 - val_accuracy: 0.6334\n",
      "Epoch 13/20\n",
      "80/80 [==============================] - 340s 4s/step - loss: 0.6365 - accuracy: 0.6316 - val_loss: 0.6158 - val_accuracy: 0.6912\n",
      "Epoch 14/20\n",
      "80/80 [==============================] - 336s 4s/step - loss: 0.6165 - accuracy: 0.6475 - val_loss: 0.6129 - val_accuracy: 0.6884\n",
      "Epoch 15/20\n",
      "80/80 [==============================] - 338s 4s/step - loss: 0.6171 - accuracy: 0.6549 - val_loss: 0.5877 - val_accuracy: 0.7139\n",
      "Epoch 16/20\n",
      "80/80 [==============================] - 301s 4s/step - loss: 0.6320 - accuracy: 0.6729 - val_loss: 0.6039 - val_accuracy: 0.7073\n",
      "Epoch 17/20\n",
      "80/80 [==============================] - 313s 4s/step - loss: 0.6172 - accuracy: 0.6830 - val_loss: 0.5881 - val_accuracy: 0.7251\n",
      "Epoch 18/20\n",
      "80/80 [==============================] - 308s 4s/step - loss: 0.6043 - accuracy: 0.6856 - val_loss: 0.5826 - val_accuracy: 0.7285\n",
      "Epoch 19/20\n",
      "80/80 [==============================] - 295s 4s/step - loss: 0.6023 - accuracy: 0.6974 - val_loss: 0.6216 - val_accuracy: 0.6683\n",
      "Epoch 20/20\n",
      "80/80 [==============================] - 312s 4s/step - loss: 0.5920 - accuracy: 0.7010 - val_loss: 0.5971 - val_accuracy: 0.7020\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_generator,\n",
    "                 steps_per_epoch=len(train_generator),\n",
    "                 epochs=epochs,\n",
    "                 validation_data=val_generator,\n",
    "                 validation_steps=len(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the loss and accuracy curves for training and validation \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(hist.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(hist.history['val_loss'], color='r', label=\"validation loss\")\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(hist.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(cm, classes,\n\u001b[1;32m      2\u001b[0m                           normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m                           title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m                           cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mBlues):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    This function prints and plots the confusion matrix.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Normalization can be applied by setting `normalize=True`.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(cm, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance\n",
    "score = model.evaluate(x= X_val, y= Y_val, batch_size=32)\n",
    "acc = score[1]\n",
    "err = 1 - acc\n",
    "print(\"Loss Value : \", score[0])\n",
    "print(\"Accuracy : \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path(\"models/model_structure-5.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"models/model_weights-5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
